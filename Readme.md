# ğŸ§  Lexi Legal RAG Backend

A fully offline Retrieval-Augmented Generation (RAG) backend that takes legal queries, retrieves relevant text from real court judgments (PDF/DOCX), and generates an answer with citations â€” using **only free and open-source tools**.

---

## âš™ï¸ Tech Stack

| Component    | Tool                                       |
| ------------ | ------------------------------------------ |
| Backend      | FastAPI                                    |
| Embeddings   | sentence-transformers (`all-MiniLM-L6-v2`) |
| Vector Store | FAISS (local)                              |
| LLM          | Ollama (`mistral` / `llama3`)              |
| Parser       | `pdfplumber`, `python-docx`                |

---

## ğŸš€ Features

- âœ… Upload legal `.pdf` / `.docx` documents
- âœ… Extract & chunk text for semantic search
- âœ… Embed using free SentenceTransformer
- âœ… Query via `/query` endpoint
- âœ… Get generated answer + source citations
- âœ… 100% offline â€” no paid APIs needed

---

## ğŸ“ Project Structure

lexi.sg-rag-backend-test/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # FastAPI app
â”‚ â”œâ”€â”€ loader.py # Document loader
â”‚ â”œâ”€â”€ embedder.py # Chunking, embeddings, FAISS logic
â”‚ â”œâ”€â”€ rag_pipeline.py # RAG logic (retrieval + LLM)
â”‚ â”œâ”€â”€ test_loader.py # Test PDF/DOCX extraction
â”‚ â”œâ”€â”€ test_rag.py # Test full pipeline
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ build_index.py # Build embeddings/index
â”‚
â”œâ”€â”€ data/ # Put your legal documents here
â”‚ â”œâ”€â”€ example.pdf
â”‚ â””â”€â”€ case.docx
â”‚
â”œâ”€â”€ vector_store/ # FAISS index + metadata
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md # You're reading it!

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repo & Set Up Virtual Environment

```bash
git clone https://github.com/<your-username>/lexi.sg-rag-backend-test
cd lexi.sg-rag-backend-test

python -m venv venv
source venv/bin/activate  # (Windows: venv\Scripts\activate)

pip install -r requirements.txt


```

### 2ï¸âƒ£ Start Ollama (Free Local LLM)

- Download and install Ollama: [https://ollama.com/download](https://ollama.com/download)
- Start the model in your terminal:

```bash
ollama run mistral
```

âœ… Ollama listens on `http://localhost:11434` and serves as an OpenAI-compatible local LLM.

- To use a different model (e.g., `llama3`):

```bash
ollama run llama3
```

Then update the model name in `rag_pipeline.py` to `"llama3"`.

---

### 3ï¸âƒ£ Prepare Documents

Place your legal files in the `/data/` folder:

```
/data/
â”œâ”€â”€ Anil Kumar v. Roop Kumar Sharma.docx
â”œâ”€â”€ Dakshin PSPCL vs. Sudesh Rani.pdf
```

---

### 4ï¸âƒ£ Build Embedding Index

```bash
python scripts/build_index.py
```

âœ… This will:

- Chunk the documents
- Embed them using sentence-transformers
- Store everything in FAISS at `/vector_store`

---

### 5ï¸âƒ£ Run the Backend

```bash
uvicorn app.main:app --reload
```

Visit the Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)

---

### ğŸ” Example: Use the `/query` Endpoint

**POST** `/query`

**Request:**

```json
{
  "query": "Is an Indian insurance company liable if a vehicle insured in India meets an accident in Nepal?"
}
```

**Response:**

```json
{
  "answer": "Yes, the insurer is liable if...",
  "citations": [
    {
      "text": "Once a vehicle is insured qua third party it is insured for all geographical areas...",
      "source": "Anil Kumar v. Roop Kumar Sharma.docx"
    },
    {
      "text": "Section 149 (3) makes the Insurance Company liable to satisfy the decree...",
      "source": "Anil Kumar v. Roop Kumar Sharma.docx"
    }
  ]
}
```

---

### ğŸ§  How the RAG Flow Works

- **Document Loading:** PDF & DOCX files are parsed to extract clean text.
- **Chunking:** Text is split into ~300-word segments with overlap.
- **Embedding:** Chunks are embedded using `all-MiniLM-L6-v2`.
- **Vector Store:** FAISS stores embeddings + metadata (file name, chunk ID).

**Query Flow:**

1. User sends a question.
2. Query is embedded and top K similar chunks are retrieved.
3. Chunks + query are sent to Ollama (LLM).
4. Final answer and citations are returned.

---

### â— Troubleshooting

- **ğŸ Error:** `openai.ChatCompletion.create` fails  
   **Fix:** You may be using `openai` version â‰¥ 1.0.  
   Downgrade to 0.28 (recommended for Ollama):

  ```bash
  pip uninstall openai
  pip install openai==0.28
  ```

  Or upgrade code to use the new client class.

- **ğŸ§  Ollama gives connection error?**  
   Make sure you ran:

  ```bash
  ollama run mistral
  ```

  Also ensure port `11434` is available.

- **ğŸ“„ Document parsing issues?**  
   Ensure your files are valid PDF/DOCX formats. Check the `scripts/build_index.py` for any parsing errors.
- **ğŸ” No results for queries?**  
   Ensure you have built the index with `python scripts/build_index.py` after adding new documents.
- **ğŸ“¦ Missing dependencies?**
  Check your `requirements.txt` file and ensure all packages are installed.
- **ğŸ“‚ Data folder empty?**
  Make sure you have placed your legal documents in the `/data/` folder.
- **â“ Other issues?**
  Check the logs for any errors and ensure all services (Ollama, FastAPI) are running correctly.

---

## ğŸ™ Acknowledgement

Special thanks to the authors of the libraries and tools used in this project.

## ğŸ“ Contact

For any inquiries, please reach out to [devx.ankitx@gmail.com](mailto:devx.ankitx@gmail.com).
